# -*- coding: utf-8 -*-
"""Assignment 5b.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TSp_JHT95iU1IVc6G8DaiLVkRtMtH1iG

# **Assignment 5b**
"""

# !pip install nltk

# Libraries required
import nltk
from nltk import ngrams
from nltk.corpus import stopwords
from nltk.corpus import movie_reviews
from nltk.tokenize import word_tokenize
from nltk import classify
from nltk import NaiveBayesClassifier
import string

# Download packages
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('movie_reviews')

stopwords_english = stopwords.words('english')

# Clean words, i.e. remove stopwords and punctuation
def clean_words(words, stopwords_english):
    words_clean = []
    for word in words:
        word = word.lower()
        if word not in stopwords_english and word not in string.punctuation:
            words_clean.append(word)
    return words_clean

# Feature extractor function for unigram
def bag_of_words(words):
    words_dictionary = dict([word, True] for word in words)
    return words_dictionary

# Feature extractor function for bigram
def bag_of_ngrams(words, n=2):
    words_ng = []
    for item in iter(ngrams(words, n)):
        words_ng.append(item)
    words_dictionary = dict([word, True] for word in words_ng)
    return words_dictionary


# Cleaning words is fine for unigrams but this can omit important words for bigrams
# We create a new stopwords list specifically for bigrams by omitting such important words
important_words = ['above', 'below', 'off', 'over', 'under', 'more', 'most', 'such', 'no', 'nor', 'not', 'only', 'so', 'than', 'too', 'very', 'just', 'but']
stopwords_english_for_bigrams = set(stopwords_english) - set(important_words)


# Define a function that extracts unigram and bigram features
def bag_of_all_words(words, n=2):
    words_clean = clean_words(words, stopwords_english)
    words_clean_for_bigrams = clean_words(words, stopwords_english_for_bigrams)

    unigram_features = bag_of_words(words_clean)
    bigram_features = bag_of_ngrams(words_clean_for_bigrams)

    all_features = unigram_features.copy()
    all_features.update(bigram_features)

    return all_features

# List with the positive reviews
pos_reviews = []
for fileid in movie_reviews.fileids('pos'):
    words = movie_reviews.words(fileid)
    pos_reviews.append(words)

# List with the negative reviews
neg_reviews = []
for fileid in movie_reviews.fileids('neg'):
    words = movie_reviews.words(fileid)
    neg_reviews.append(words)

# Positive reviews feature set
pos_reviews_set = []
for words in pos_reviews:
    pos_reviews_set.append((bag_of_all_words(words), 'pos'))

# Negative reviews feature set
neg_reviews_set = []
for words in neg_reviews:
    neg_reviews_set.append((bag_of_all_words(words), 'neg'))

# Radomize pos_reviews_set and neg_reviews_set
# Doing so will output different accuracy result everytime we run the program
from random import shuffle
shuffle(pos_reviews_set)
shuffle(neg_reviews_set)

test_set = pos_reviews_set[:200] + neg_reviews_set[:200]
train_set = pos_reviews_set[200:] + neg_reviews_set[200:]

classifier = NaiveBayesClassifier.train(train_set)

accuracy = classify.accuracy(classifier, test_set)
print(f'Classifier accuracy: {accuracy}')

custom_review = "Starts as buddy movie then derails into a patchwork of clich√©s"
custom_review_tokens = word_tokenize(custom_review)
custom_review_set = bag_of_all_words(custom_review_tokens)
print (f'Classification of the review \n[{custom_review}]\n as: {classifier.classify(custom_review_set)}')

# probability result
prob_result = classifier.prob_classify(custom_review_set)
print (f'Probability of a negative review: {round((prob_result.prob("neg"))*100, 2)}%')
print (f'Probability of a positive review: {round((prob_result.prob("pos"))*100, 2)}%')


print('--------------------------------------------------------')

custom_review = "Crowe & Gosling Shine In This Winning Buddy-Comedy!"
custom_review_tokens = word_tokenize(custom_review)
custom_review_set = bag_of_all_words(custom_review_tokens)
print (f'Classification of the review \n[{custom_review}]\n as: {classifier.classify(custom_review_set)}')

# probability result
prob_result = classifier.prob_classify(custom_review_set)
print (f'Probability of a negative review: {round((prob_result.prob("neg"))*100, 2)}%')
print (f'Probability of a positive review: {round((prob_result.prob("pos"))*100, 2)}%')


print('--------------------------------------------------------')

custom_review = "Disjointed, uneven, sporadically funny."
custom_review_tokens = word_tokenize(custom_review)
custom_review_set = bag_of_all_words(custom_review_tokens)
print (f'Classification of the review \n[{custom_review}]\n as: {classifier.classify(custom_review_set)}')

# probability result
prob_result = classifier.prob_classify(custom_review_set)
print (f'Probability of a negative review: {round((prob_result.prob("neg"))*100, 2)}%')
print (f'Probability of a positive review: {round((prob_result.prob("pos"))*100, 2)}%')